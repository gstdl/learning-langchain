{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_api_key = os.environ[\"OPENAI_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " at age 100?\n",
      "\n",
      "Yes, it is possible to have fun at age 100. Many people at this age still have active social lives, engage in hobbies and activities they enjoy, and have a positive outlook on life. Some may even continue to work or volunteer, travel, and try new things. Age does not have to limit one's ability to have fun and enjoy life. \n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "question = 'Can you still have fun'\n",
    "\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting and chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template='You are a Biology Professor, answer the question. {question}')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"You are a Biology Professor, answer the question. {question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The pancreas is a vital organ in the body that plays an important role in digestion and maintaining blood sugar levels. It is located in the upper abdomen, behind the stomach.\n",
      "\n",
      "The main function of the pancreas is to produce and release digestive enzymes and hormones that help break down food and regulate blood sugar levels. The digestive enzymes produced by the pancreas help to break down carbohydrates, proteins, and fats in food, which are then absorbed by the small intestine. The hormones produced by the pancreas, such as insulin and glucagon, help to regulate the amount of sugar in the blood. Insulin helps to lower blood sugar levels, while glucagon helps to increase blood sugar levels.\n",
      "\n",
      "In addition to its role in digestion and blood sugar regulation, the pancreas also has an important role in the endocrine system. It produces and releases several hormones, including insulin, glucagon, somatostatin, and pancreatic polypeptide, which help to regulate various bodily functions, such as metabolism, growth, and digestion.\n",
      "\n",
      "Overall, the pancreas is a crucial organ in the body that helps to maintain proper digestion and regulate blood sugar levels, making it essential for overall health and well-being. \n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "template = \"You are a Biology Professor, answer the question. {question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\"question\": \"What's the function of Pancreas?\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sistem: Dari makhluk yang paling indah kita menginginkan peningkatan,\n",
      "Agar bunga kecantikan itu tidak pernah mati,\n",
      "Tetapi seperti yang lebih tua harus mati oleh waktu,\n",
      "Ahli waris yang lembutnya dapat membawa ingatannya:\n",
      "Tetapi kamu terikat pada mata indahmu sendiri,\n",
      "Menyediakan api cahayamu dengan bahan bakar yang substansial,\n",
      "Membuat kelaparan di tempat kelimpahan berada,\n",
      "Dirimu musuhmu, terlalu kejam pada dirimu yang manis:\n",
      "Kamu yang sekarang menjadi hiasan dunia yang segar,\n",
      "Dan hanya pengumuman untuk musim semi yang mewah,\n",
      "Dalam kuncupmu sendiri kamu mengubur kepuasanmu,\n",
      "Dan orang yang lembut yang menyia-nyiakan dengan kikir:\n",
      "Sayangilah dunia, atau menjadi rakus ini,\n",
      "Untuk makan hak dunia, oleh kubur dan dirimu.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates the {input_language} to {output_language}\"),\n",
    "    (\"user\", \"Translate the following passage: {passage}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"input_language\": \"English\", \n",
    "    \"output_language\": \"Bahasa Indonesia\", \n",
    "    \"passage\": \"\"\"From fairest creatures we desire increase,\n",
    "That thereby beauty's rose might never die,\n",
    "But as the riper should by time decease,\n",
    "His tender heir might bear his memory:\n",
    "But thou contracted to thine own bright eyes,\n",
    "Feed'st thy light's flame with self-substantial fuel,\n",
    "Making a famine where abundance lies,\n",
    "Thy self thy foe, to thy sweet self too cruel:\n",
    "Thou that art now the world's fresh ornament,\n",
    "And only herald to the gaudy spring,\n",
    "Within thine own bud buriest thy content,\n",
    "And tender churl mak'st waste in niggarding:\n",
    "Pity the world, or else this glutton be,\n",
    "To eat the world's due, by the grave and thee.\"\"\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Describe a language model in one sentence\n",
      "A: A language model is a statistical model that predicts the probability of a sequence of words in a natural language.\n",
      "Q: Describe it again using less words\n",
      "A: Describe again with fewer words\n",
      "Q: Describe it again fewer words but at least one word\n",
      "A: Summarize concisely\n",
      "Q: What did I first ask you? I forgot.\n",
      "A: I'm sorry, I cannot remember the first thing you asked me as I am a computer AI and do not have the ability to retain information in the same way as a human. Is there something else you would like to ask me?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "for question in [\n",
    "    \"Describe a language model in one sentence\",\n",
    "    \"Describe it again using less words\",\n",
    "    \"Describe it again fewer words but at least one word\",\n",
    "    \"What did I first ask you? I forgot.\",\n",
    "]:\n",
    "    output = llm.invoke(question)\n",
    "    print(\"Q:\", question)\n",
    "    print(\"A:\", output.lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Describe a language model in one sentence\n",
      "A: A language model is a statistical model that predicts the probability of a sequence of words occurring in a given context.\n",
      "Q: Describe it again using less words\n",
      "A: A language model predicts the likelihood of words appearing together.\n",
      "Q: Describe it again fewer words but at least one word\n",
      "A: Predicts word sequences\n",
      "Q: What did I first ask you? I forgot.\n",
      "A: You first asked me to describe a language model in one sentence.\n",
      "[HumanMessage(content='Describe a language model in one sentence'), AIMessage(content='A language model is a statistical model that predicts the probability of a sequence of words occurring in a given context.'), HumanMessage(content='Describe it again using less words'), AIMessage(content='A language model predicts the likelihood of words appearing together.'), HumanMessage(content='Describe it again fewer words but at least one word'), AIMessage(content='Predicts word sequences'), HumanMessage(content='What did I first ask you? I forgot.'), AIMessage(content='You first asked me to describe a language model in one sentence.')]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "for question in [\n",
    "    \"Describe a language model in one sentence\",\n",
    "    \"Describe it again using less words\",\n",
    "    \"Describe it again fewer words but at least one word\",\n",
    "    \"What did I first ask you? I forgot.\",\n",
    "]:\n",
    "    inputs = {\"input\": question}\n",
    "    output = chain.invoke(inputs)\n",
    "    print(\"Q:\", question)\n",
    "    print(\"A:\", output.content)\n",
    "    memory.save_context(inputs, {\"output\": output.content})\n",
    "\n",
    "print(memory.load_memory_variables({})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it. This happens when a massive star collapses in on itself, creating a singularity at the center. Black holes can come in different sizes, from stellar black holes that are a few times the mass of our sun, to supermassive black holes that are millions or even billions of times more massive. They are fascinating objects in the universe that can warp spacetime and have a significant impact on the surrounding environment.\n",
      "Using MATH\n",
      "A path integral, also known as a line integral, is a concept in mathematics that involves integrating a function along a curve or path in a multi-dimensional space. It is used in various fields such as physics, particularly in quantum mechanics, where it represents the sum of all possible paths that a particle can take between two points. The path integral allows for a more comprehensive understanding of the behavior of particles and systems in complex environments.\n",
      "Using MATH\n",
      "To find x, we need to solve the equation 2x + 3 = 32.\n",
      "\n",
      "Subtract 3 from both sides:\n",
      "2x = 29\n",
      "\n",
      "Divide both sides by 2:\n",
      "x = 29/2\n",
      "\n",
      "Therefore, x = 14.5.\n",
      "Using PHYSICS\n",
      "Barack Obama is the 44th President of the United States, serving two terms from 2009 to 2017. He was the first African American to hold the office of President.\n"
     ]
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "\n",
    "def prompt_router(input):\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What's a black hole\"))\n",
    "print(chain.invoke(\"What's a path integral\"))\n",
    "print(chain.invoke(\"Find x, 2x + 3 = 32\"))\n",
    "print(chain.invoke(\"Who's Barack Obama?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/langchain-demo/lib/python3.12/site-packages/langchain_community/vectorstores/pgvector.py:289: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata.Please note that filtering operators have been changed when using JSOB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create adb migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  warn_deprecated(\n",
      "Created a chunk of size 177, which is longer than the specified 150\n",
      "Created a chunk of size 229, which is longer than the specified 150\n",
      "Created a chunk of size 233, which is longer than the specified 150\n",
      "Created a chunk of size 185, which is longer than the specified 150\n",
      "Created a chunk of size 203, which is longer than the specified 150\n",
      "Created a chunk of size 299, which is longer than the specified 150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "page_content='Q: What are the hours of operation for Bella Vista?\\nA: Bella Vista is open from 11 a.m. to 11 p.m. from Monday to Saturday. On Sundays, we welcome guests from 12 p.m. to 10 p.m.' metadata={'source': './bella-vista.txt'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_added': 0, 'num_updated': 0, 'num_skipped': 7, 'num_deleted': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain.schema import Document\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "CONNECTION_STRING = \"postgresql+psycopg2://admin:admin@127.0.0.1:5455/vectordb\"\n",
    "COLLECTION_NAME = \"vectordb\"\n",
    "\n",
    "\n",
    "vectorstore = PGVector.from_documents(\n",
    "    [],\n",
    "    embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING\n",
    ")\n",
    "\n",
    "loader = TextLoader(\"./bella-vista.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=150, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(len(docs))\n",
    "print(docs[0])\n",
    "\n",
    "\n",
    "# Update namespace to reflect PGVector\n",
    "namespace = f\"pgvector/{COLLECTION_NAME}\"\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace, db_url=CONNECTION_STRING\n",
    ")\n",
    "\n",
    "# Create schema for the record manager\n",
    "record_manager.create_schema()\n",
    "\n",
    "index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    # cleanup=None, # handles some deduplication\n",
    "    # cleanup=\"incremental\", # does not handle deduplication\n",
    "    cleanup=\"full\", # deletes everything then indexes the new documents\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Q: What are the hours of operation for Bella Vista?\\nA: Bella Vista is open from 11 a.m. to 11 p.m. from Monday to Saturday. On Sundays, we welcome guests from 12 p.m. to 10 p.m.', metadata={'source': './bella-vista.txt'}),\n",
       "  0.1955708411406436),\n",
       " (Document(page_content='Q: Do I need a reservation for Bella Vista?\\nA: While walk-ins are always welcome, we recommend making a reservation, especially during weekends and holidays, to ensure a seamless dining experience.', metadata={'source': './bella-vista.txt'}),\n",
       "  0.20732310492145822),\n",
       " (Document(page_content='Q: Do you offer vegetarian or vegan options at Bella Vista?\\nA: Absolutely! Bella Vista boasts a diverse menu that includes a variety of vegetarian and vegan dishes. Our chefs are also happy to customize dishes based on dietary needs.', metadata={'source': './bella-vista.txt'}),\n",
       "  0.23744431933976706),\n",
       " (Document(page_content='Q: Can I book private events at Bella Vista?\\nA: Certainly! Bella Vista has a private dining area perfect for events, parties, or corporate gatherings. We also offer catering services for off-site events.', metadata={'source': './bella-vista.txt'}),\n",
       "  0.24587078731255063)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(\"when does the restaurant open?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Q: Do I need a reservation for Bella Vista?\\nA: While walk-ins are always welcome, we recommend making a reservation, especially during weekends and holidays, to ensure a seamless dining experience.', metadata={'source': './bella-vista.txt'}),\n",
       " Document(page_content='Q: Can I book private events at Bella Vista?\\nA: Certainly! Bella Vista has a private dining area perfect for events, parties, or corporate gatherings. We also offer catering services for off-site events.', metadata={'source': './bella-vista.txt'}),\n",
       " Document(page_content=\"Q: Is Bella Vista family-friendly?\\nA: Yes, Bella Vista is a family-friendly establishment. We have a dedicated kids' menu and offer high chairs and booster seats for our younger guests.\", metadata={'source': './bella-vista.txt'}),\n",
       " Document(page_content=\"Q: What's the ambiance like at Bella Vista?\\nA: Bella Vista boasts a cozy and elegant setting, with ambient lighting, comfortable seating, and a stunning view of the city skyline. Whether you're looking for a romantic dinner or a casual meal with friends, Bella Vista provides the perfect atmosphere.\", metadata={'source': './bella-vista.txt'}),\n",
       " Document(page_content='Q: Do you offer vegetarian or vegan options at Bella Vista?\\nA: Absolutely! Bella Vista boasts a diverse menu that includes a variety of vegetarian and vegan dishes. Our chefs are also happy to customize dishes based on dietary needs.', metadata={'source': './bella-vista.txt'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "retriever.get_relevant_documents(query=\"reservation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/langchain-demo/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "from typing import Optional\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
    "\n",
    "class CustomSearchTool(BaseTool):\n",
    "    name = \"bella vista restaurant information search\"\n",
    "    description = \"useful for when you need to answer questions about bella vista restaurant\"\n",
    "\n",
    "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        store = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "        docs = store.get_relevant_documents(query)\n",
    "        text_list = [doc.page_content for doc in docs]\n",
    "        return \"\\n\".join(text_list)\n",
    "\n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")\n",
    "\n",
    "tools = [CustomSearchTool()]\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "agent_chain = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, \n",
    "    verbose=True, \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> bella vista restaurant information search: useful for when you need to answer questions about bella vista restaurant\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\\\\\ The action to take. Must be one of bella vista restaurant information search\\n    \"action_input\": string \\\\\\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n"
     ]
    }
   ],
   "source": [
    "print(agent_chain.agent.llm_chain.prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/langchain-demo/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"bella vista restaurant information search\",\n",
      "    \"action_input\": \"Bella Vista opening hours on Saturday\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mQ: What are the hours of operation for Bella Vista?\n",
      "A: Bella Vista is open from 11 a.m. to 11 p.m. from Monday to Saturday. On Sundays, we welcome guests from 12 p.m. to 10 p.m.\n",
      "Q: Do I need a reservation for Bella Vista?\n",
      "A: While walk-ins are always welcome, we recommend making a reservation, especially during weekends and holidays, to ensure a seamless dining experience.\n",
      "Q: Is Bella Vista family-friendly?\n",
      "A: Yes, Bella Vista is a family-friendly establishment. We have a dedicated kids' menu and offer high chairs and booster seats for our younger guests.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Bella Vista is open from 11 a.m. to 11 p.m. from Monday to Saturday. On Sundays, they welcome guests from 12 p.m. to 10 p.m. Walk-ins are welcome, but making a reservation is recommended, especially during weekends and holidays. Bella Vista is family-friendly and offers a dedicated kids' menu.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Bella Vista is open from 11 a.m. to 11 p.m. from Monday to Saturday. On Sundays, they welcome guests from 12 p.m. to 10 p.m. Walk-ins are welcome, but making a reservation is recommended, especially during weekends and holidays. Bella Vista is family-friendly and offers a dedicated kids' menu.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input=\"When is Bella Vista open on Saturday?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"bella vista restaurant information search\",\n",
      "    \"action_input\": \"activities or events at Bella Vista restaurant\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mQ: Can I book private events at Bella Vista?\n",
      "A: Certainly! Bella Vista has a private dining area perfect for events, parties, or corporate gatherings. We also offer catering services for off-site events.\n",
      "Q: What's the ambiance like at Bella Vista?\n",
      "A: Bella Vista boasts a cozy and elegant setting, with ambient lighting, comfortable seating, and a stunning view of the city skyline. Whether you're looking for a romantic dinner or a casual meal with friends, Bella Vista provides the perfect atmosphere.\n",
      "Q: What type of cuisine does Bella Vista serve?\n",
      "A: Bella Vista offers a delightful blend of Mediterranean and contemporary American cuisine. We pride ourselves on using the freshest ingredients, many of which are sourced locally.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Bella Vista restaurant offers a delightful blend of Mediterranean and contemporary American cuisine, with a cozy and elegant setting, ambient lighting, comfortable seating, and a stunning view of the city skyline.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bella Vista restaurant offers a delightful blend of Mediterranean and contemporary American cuisine, with a cozy and elegant setting, ambient lighting, comfortable seating, and a stunning view of the city skyline.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input=\"Any suggestions what to do there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
